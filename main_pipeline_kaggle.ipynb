{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        # Combine YrSold and MoSold into a datetime format\n",
    "        X['Date'] = pd.to_datetime(X['YrSold'].astype(str) + X['MoSold'].astype(str).str.zfill(2), format='%Y%m')\n",
    "        # Convert datetime to Unix timestamp in seconds\n",
    "        X['Date'] = X['Date'].astype('int64') // 10**9\n",
    "        # Drop the original 'MoSold' and 'YrSold' columns\n",
    "        X.drop(['MoSold', 'YrSold'], axis=1, inplace=True) \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom transformer for encoding categorical columns\n",
    "class LEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mappings):\n",
    "        self.mappings = mappings\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for column_name, mapping_dict in self.mappings.items():\n",
    "            if column_name in X.columns:\n",
    "                # Handle null values by filling with 'NA'\n",
    "                X[column_name] = X[column_name].fillna('NA')\n",
    "                # Create the encoded column\n",
    "                X[column_name + '_encoded'] = X[column_name].map(mapping_dict)\n",
    "                # Drop the original column\n",
    "                X = X.drop([column_name], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This custom OHEncoder ensure the return object to be dataframe\n",
    "class OHEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, handle_unknown='ignore'):\n",
    "        self.columns = columns\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.encoder = OneHotEncoder(handle_unknown=handle_unknown, sparse_output=False)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the encoder on the specified columns\n",
    "        if self.columns is None:\n",
    "            self.columns = X.columns\n",
    "        self.encoder.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform the specified columns\n",
    "        X_transformed = X.copy()\n",
    "        # Apply the encoder\n",
    "        encoded_array = self.encoder.transform(X[self.columns])\n",
    "        # Create DataFrame with appropriate column names\n",
    "        encoded_df = pd.DataFrame(\n",
    "            encoded_array, \n",
    "            columns=self.encoder.get_feature_names_out(self.columns),\n",
    "            index=X.index\n",
    "        )\n",
    "        # Drop the original columns and concatenate the new DataFrame\n",
    "        X_transformed = X_transformed.drop(columns=self.columns)\n",
    "        X_transformed = pd.concat([X_transformed, encoded_df], axis=1)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Feature stripper \n",
    "class StripColumn(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns_file = 'retained_columns.txt'\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        X = X.copy()\n",
    "        numeric_cols = X.select_dtypes(include=[int, float]).columns\n",
    "        cols_to_keep = numeric_cols[X[numeric_cols].sum() >= 10]\n",
    "        \n",
    "        # Save column indices to a text file\n",
    "        with open(self.columns_file, 'w') as f:\n",
    "            for index in range(len(cols_to_keep)):\n",
    "                f.write(f\"{index}\\n\")\n",
    "        \n",
    "        # Save column names separately if needed for reference\n",
    "        self.column_names = cols_to_keep.tolist()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        X = X.copy()\n",
    "        \n",
    "        # Check if the columns file exists\n",
    "        if os.path.exists(self.columns_file):\n",
    "            # Load the indices to keep from the text file\n",
    "            with open(self.columns_file, 'r') as f:\n",
    "                indices_to_keep = [int(line.strip()) for line in f]\n",
    "            \n",
    "            # Use indices to select columns\n",
    "            if hasattr(self, 'column_names'):\n",
    "                # Use saved column names to select by indices\n",
    "                cols_to_keep = [self.column_names[i] for i in indices_to_keep]\n",
    "            else:\n",
    "                # If names are not available, directly use indices\n",
    "                cols_to_keep = X.columns[indices_to_keep]\n",
    "            \n",
    "            X = X[cols_to_keep]\n",
    "        else:\n",
    "            print(\"Column file not found. Transforming with all columns.\")\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform the SalePrice\n",
    "class LogTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name='SalePrice'):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if self.column_name in X.columns:\n",
    "            X[f'Log{self.column_name}'] = np.log(X[self.column_name])\n",
    "            X = X.drop([self.column_name], axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Column {self.column_name} not found in the DataFrame\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumn(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns: list):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting necessary for this transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"X should be a pandas DataFrame\")\n",
    "\n",
    "        # Identify which columns to drop and which do not exist\n",
    "        existing_columns = [col for col in self.columns if col in X.columns]\n",
    "        non_existent_columns = [col for col in self.columns if col not in X.columns]\n",
    "\n",
    "        # Print out which columns are not found\n",
    "        if non_existent_columns:\n",
    "            print(f\"Columns not found in DataFrame: {non_existent_columns}\")\n",
    "\n",
    "        # Drop the existing columns\n",
    "        X_transformed = X.drop(columns=existing_columns)\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "        self.imputer = SimpleImputer(strategy=strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = X.copy()\n",
    "        # Transform and convert to DataFrame\n",
    "        imputed_array = self.imputer.transform(X)\n",
    "        return pd.DataFrame(imputed_array, columns=X.columns, index=X.index)\n",
    "    \n",
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = X.copy()\n",
    "        # Transform and convert to DataFrame\n",
    "        scaled_array = self.scaler.transform(X)\n",
    "        return pd.DataFrame(scaled_array, columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for categorical columns\n",
    "categorical_mappings = {\n",
    "    'PoolQC': {'NA': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "    'Fence': {'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4},\n",
    "    'GarageCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'GarageQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'GarageFinish': {'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n",
    "    'FireplaceQu': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'Functional': {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7},\n",
    "    'KitchenQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "    'HeatingQC': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "    'BsmtFinType2': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'BsmtFinType1': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'BsmtExposure': {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'BsmtQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'BsmtCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'ExterCond': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "    'ExterQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
    "    'LandSlope': {'Sev': 0, 'Mod': 1, 'Gtl': 2},\n",
    "    'LotShape': {'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3}\n",
    "}\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_columns = [\n",
    "    'SaleCondition', 'SaleType', 'PavedDrive', 'Electrical', 'GarageType', 'CentralAir', \n",
    "    'MasVnrType', 'Foundation', 'Heating', 'MiscFeature', 'MSSubClass', 'MSZoning', \n",
    "    'Street', 'Alley', 'LandContour', 'Utilities', 'LotConfig', 'Neighborhood', \n",
    "    'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', \n",
    "    'Exterior1st', 'Exterior2nd'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessing pipeline\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('dropId', DropColumn(columns=['Id'])),\n",
    "    ('date_converter', DateTransformer()),\n",
    "    ('categorical_encoder', LEncoder(categorical_mappings)),\n",
    "    ('onehot_encoder', OHEncoder(columns=categorical_columns)),\n",
    "    (\"ImputerMedian\", CustomImputer(strategy='median')),\n",
    "    ('strip_column', StripColumn()),\n",
    "    ('standard_scaler', CustomScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('dataset/train.csv')\n",
    "df_test = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df_train['SalePrice']\n",
    "y = np.log(df_train['SalePrice'])\n",
    "df_train = df_train.drop('SalePrice', axis=1)\n",
    "combined_df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = df_train.shape[0]\n",
    "\n",
    "# Preprocess the combined feature set | p = 'preprocessed'\n",
    "p_train = preprocessor.fit_transform(combined_df)\n",
    "\n",
    "# Get the unlabeled test feature set out\n",
    "p_test = p_train.iloc[train_size:, :] \n",
    "\n",
    "# Get the labeled feature set\n",
    "X = p_train.iloc[:train_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=247)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest Regressor': RandomForestRegressor(n_estimators=500, max_depth=2),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=500, max_depth=2),\n",
    "    'XGBoost Regressor': XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=6, min_child_weight=1, gamma=0,\n",
    "                                        subsample=0.8, colsample_bytree=0.8, objective='reg:squarederror', eval_metric='rmse'),\n",
    "    'LGBM Regressor': LGBMRegressor(n_estimators=900, learning_rate=0.1, max_depth=1, verbosity=-1)\n",
    "}\n",
    "\n",
    "# models = {\n",
    "#     'Random Forest Regressor': RandomForestRegressor(n_estimators=500, max_depth=2),\n",
    "#     'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=500, max_depth=2),\n",
    "#     'XGBoost Regressor': XGBRegressor(learning_rate=0.1, max_depth=2, n_estimators=500),\n",
    "#     'LGBM Regressor': LGBMRegressor(n_estimators=900, learning_rate=0.1, max_depth=1, verbosity=-1)\n",
    "# }\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "    # Ensure that x is a NumPy array for element-wise operations\n",
    "    x = np.asarray(x)\n",
    "    # Clip values to avoid log(0) and negative values\n",
    "    x_clipped = np.clip(x, a_min=1e-10, a_max=None)\n",
    "    return np.log(x_clipped)\n",
    "\n",
    "# Calculate RMSE on the log scale\n",
    "def calculate_rmse_log(y_true, y_pred):\n",
    "    # Apply safe_log to avoid issues with log(0)\n",
    "    y_true_log = safe_log(y_true)\n",
    "    y_pred_log = safe_log(y_pred)\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if np.any(np.isnan(y_pred_log)):\n",
    "        raise ValueError(\"Log transformation resulted in NaN values.\")\n",
    "    return np.sqrt(mean_squared_error(y_true_log, y_pred_log)) # Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor RMSE: 0.01660677\n",
      "Gradient Boosting Regressor RMSE: 0.01088696\n",
      "XGBoost Regressor RMSE: 0.01027249\n",
      "LGBM Regressor RMSE: 0.01036735\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse_log = calculate_rmse_log(y_test, y_pred)\n",
    "    results[name] = rmse_log\n",
    "    print(f'{name} RMSE: {rmse_log:.8f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Perform GridSearchCV with the custom scoring function\u001b[39;00m\n\u001b[0;32m     26\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mrmse_log_scorer)\n\u001b[1;32m---> 27\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Output the best parameters\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:895\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    893\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 895\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    899\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\xgboost\\sklearn.py:1090\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1079\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m (\n\u001b[0;32m   1082\u001b[0m     model,\n\u001b[0;32m   1083\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1089\u001b[0m )\n\u001b[1;32m-> 1090\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1091\u001b[0m     params,\n\u001b[0;32m   1092\u001b[0m     train_dmatrix,\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1094\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   1095\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1096\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[0;32m   1097\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[0;32m   1098\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   1099\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1100\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1101\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1102\u001b[0m )\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:2047\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtrain, DMatrix):\n\u001b[0;32m   2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid training matrix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dtrain)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2050\u001b[0m     _check_call(\n\u001b[0;32m   2051\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[0;32m   2052\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   2053\u001b[0m         )\n\u001b[0;32m   2054\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:2935\u001b[0m, in \u001b[0;36mBooster._assign_dmatrix_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2933\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types \u001b[38;5;241m=\u001b[39m ft\n\u001b[1;32m-> 2935\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(fn)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:2938\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[1;34m(self, feature_names)\u001b[0m\n\u001b[0;32m   2937\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_names: Optional[FeatureNames]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2941\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:1997\u001b[0m, in \u001b[0;36mBooster.feature_names\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_names\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[FeatureNames]:\n\u001b[0;32m   1993\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Feature names for this booster.  Can be directly set by input data or by\u001b[39;00m\n\u001b[0;32m   1994\u001b[0m \u001b[38;5;124;03m    assignment.\u001b[39;00m\n\u001b[0;32m   1995\u001b[0m \n\u001b[0;32m   1996\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_feature_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:1947\u001b[0m, in \u001b[0;36mBooster._get_feature_info\u001b[1;34m(self, field)\u001b[0m\n\u001b[0;32m   1944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1946\u001b[0m _check_call(\n\u001b[1;32m-> 1947\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mXGBoosterGetStrFeatureInfo(\n\u001b[0;32m   1948\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   1949\u001b[0m         c_str(field),\n\u001b[0;32m   1950\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(length),\n\u001b[0;32m   1951\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(sarr),\n\u001b[0;32m   1952\u001b[0m     )\n\u001b[0;32m   1953\u001b[0m )\n\u001b[0;32m   1954\u001b[0m feature_info \u001b[38;5;241m=\u001b[39m from_cstr_to_pystr(sarr, length)\n\u001b[0;32m   1955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature_info \u001b[38;5;28;01mif\u001b[39;00m feature_info \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define a custom scoring function for RMSE on log-transformed data\n",
    "def rmse_log_transform(y_true, y_pred):\n",
    "    # Calculate RMSE on the log-transformed scale\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n",
    "\n",
    "# Create a custom scorer\n",
    "rmse_log_scorer = make_scorer(rmse_log_transform, greater_is_better=False)\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 1000],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'min_child_weight': [1, 5],\n",
    "    'gamma': [0, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "model = XGBRegressor(objective='reg:squarederror', eval_metric='rmse')\n",
    "\n",
    "# Perform GridSearchCV with the custom scoring function\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring=rmse_log_scorer)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse_log = calculate_rmse_log(y_test, y_pred)\n",
    "print(f'Ensemble Model RMSE: {rmse_log:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model RMSE: 0.01008863\n"
     ]
    }
   ],
   "source": [
    "# For ensemble models\n",
    "ensemble_model = VotingRegressor(\n",
    "    estimators=[\n",
    "        # ('rf', RandomForestRegressor(n_estimators=500)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=500)),\n",
    "        ('xgb', XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=6, min_child_weight=1, gamma=0,\n",
    "                                        subsample=0.8, colsample_bytree=0.8, objective='reg:squarederror', eval_metric='rmse')),\n",
    "        ('lgbm', LGBMRegressor(n_estimators=500, learning_rate=0.1, max_depth=2, verbosity=-1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "ensemble_pipeline = Pipeline(steps=[\n",
    "        ('model', ensemble_model)\n",
    "    ])\n",
    "\n",
    "ensemble_pipeline = ensemble_pipeline.fit(X_train, y_train)\n",
    "y_pred = ensemble_pipeline.predict(X_test)\n",
    "rmse_log = calculate_rmse_log(y_test, y_pred)\n",
    "print(f'Ensemble Model RMSE: {rmse_log:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousePriceModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HousePriceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        self.fc7 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "model = HousePriceModel(X_train_tensor.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3.4910226887778233\n",
      "Epoch 2/50, Loss: 0.3277937565979205\n",
      "Epoch 3/50, Loss: 0.084050542037738\n",
      "Epoch 4/50, Loss: 0.025240929846308734\n",
      "Epoch 5/50, Loss: 0.013432219305908993\n",
      "Epoch 6/50, Loss: 0.008623741763202767\n",
      "Epoch 7/50, Loss: 0.006022345103127391\n",
      "Epoch 8/50, Loss: 0.004446564413803189\n",
      "Epoch 9/50, Loss: 0.003408696796548994\n",
      "Epoch 10/50, Loss: 0.0026156002300252255\n",
      "Epoch 11/50, Loss: 0.0020991578440819132\n",
      "Epoch 12/50, Loss: 0.00175849802326411\n",
      "Epoch 13/50, Loss: 0.0014099925603276414\n",
      "Epoch 14/50, Loss: 0.0012027478200922672\n",
      "Epoch 15/50, Loss: 0.0011398931107434787\n",
      "Epoch 16/50, Loss: 0.0009477206438436712\n",
      "Epoch 17/50, Loss: 0.0008236818738575829\n",
      "Epoch 18/50, Loss: 0.000679960652058454\n",
      "Epoch 19/50, Loss: 0.0005503415074934693\n",
      "Epoch 20/50, Loss: 0.0004475177924369315\n",
      "Epoch 21/50, Loss: 0.00041505947286312123\n",
      "Epoch 22/50, Loss: 0.0003586512925961104\n",
      "Epoch 23/50, Loss: 0.0003276999157510306\n",
      "Epoch 24/50, Loss: 0.000296505961542655\n",
      "Epoch 25/50, Loss: 0.000249366658911305\n",
      "Epoch 26/50, Loss: 0.00023069466566850774\n",
      "Epoch 27/50, Loss: 0.000215640394200914\n",
      "Epoch 28/50, Loss: 0.00021696206385968253\n",
      "Epoch 29/50, Loss: 0.0001959881575251194\n",
      "Epoch 30/50, Loss: 0.00016336482842330282\n",
      "Epoch 31/50, Loss: 0.0001494795287726447\n",
      "Epoch 32/50, Loss: 0.00018528967561188007\n",
      "Epoch 33/50, Loss: 0.0002527624590009892\n",
      "Epoch 34/50, Loss: 0.0002604574949241635\n",
      "Epoch 35/50, Loss: 0.0001629727607666466\n",
      "Epoch 36/50, Loss: 0.00011045985516638642\n",
      "Epoch 37/50, Loss: 0.00010353613623393405\n",
      "Epoch 38/50, Loss: 0.00011181088522885387\n",
      "Epoch 39/50, Loss: 0.0001142736984643546\n",
      "Epoch 40/50, Loss: 0.0001231530965780717\n",
      "Epoch 41/50, Loss: 0.00012131256785084445\n",
      "Epoch 42/50, Loss: 0.00012215399730944467\n",
      "Epoch 43/50, Loss: 0.0003286128425985991\n",
      "Epoch 44/50, Loss: 0.0015171835912042297\n",
      "Epoch 45/50, Loss: 0.001306517619492584\n",
      "Epoch 46/50, Loss: 0.001030025786205538\n",
      "Epoch 47/50, Loss: 0.000673703444925578\n",
      "Epoch 48/50, Loss: 0.0004369644019262571\n",
      "Epoch 49/50, Loss: 0.0003502745752676243\n",
      "Epoch 50/50, Loss: 0.0002895911131513697\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            targets_log = torch.log(targets + 1)  # Apply log transformation\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets_log)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.5558704137802124\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for inputs, target in test_loader:\n",
    "            output = model(inputs)\n",
    "            predictions.append(output.numpy())\n",
    "            targets.append(target.numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        targets = np.concatenate(targets)\n",
    "        \n",
    "        # Compute RMSE between log-transformed predictions and targets\n",
    "        epsilon = 1e-8\n",
    "        log_predictions = np.log(predictions + epsilon)  # Apply log transformation\n",
    "        log_targets = np.log(targets + epsilon)\n",
    "        mse = np.mean((log_predictions - log_targets) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f'Test RMSE: {rmse}')\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 103.60258| val_0_mse: 63.58573913574219|  0:00:01s\n",
      "epoch 1  | loss: 24.36324| val_0_mse: 23.579059600830078|  0:00:01s\n",
      "epoch 2  | loss: 4.38717 | val_0_mse: 7.3284101486206055|  0:00:02s\n",
      "epoch 3  | loss: 1.41206 | val_0_mse: 3.1465799808502197|  0:00:02s\n",
      "epoch 4  | loss: 0.85694 | val_0_mse: 10.383910179138184|  0:00:03s\n",
      "epoch 5  | loss: 0.43355 | val_0_mse: 4.817409992218018|  0:00:04s\n",
      "epoch 6  | loss: 0.31852 | val_0_mse: 14.489990234375|  0:00:04s\n",
      "epoch 7  | loss: 0.36035 | val_0_mse: 7.753880023956299|  0:00:05s\n",
      "epoch 8  | loss: 0.20058 | val_0_mse: 5.5832600593566895|  0:00:06s\n",
      "epoch 9  | loss: 0.17102 | val_0_mse: 4.623380184173584|  0:00:06s\n",
      "epoch 10 | loss: 0.19933 | val_0_mse: 1.8931699991226196|  0:00:07s\n",
      "epoch 11 | loss: 0.21299 | val_0_mse: 2.7609400749206543|  0:00:07s\n",
      "epoch 12 | loss: 0.17574 | val_0_mse: 3.557490110397339|  0:00:08s\n",
      "epoch 13 | loss: 0.16404 | val_0_mse: 1.771720051765442|  0:00:09s\n",
      "epoch 14 | loss: 0.13178 | val_0_mse: 1.5937600135803223|  0:00:09s\n",
      "epoch 15 | loss: 0.12625 | val_0_mse: 0.8987900018692017|  0:00:10s\n",
      "epoch 16 | loss: 0.13735 | val_0_mse: 0.6523000001907349|  0:00:10s\n",
      "epoch 17 | loss: 0.1291  | val_0_mse: 0.6728799939155579|  0:00:11s\n",
      "epoch 18 | loss: 0.1132  | val_0_mse: 0.667140007019043|  0:00:12s\n",
      "epoch 19 | loss: 0.10867 | val_0_mse: 1.024649977684021|  0:00:12s\n",
      "epoch 20 | loss: 0.12692 | val_0_mse: 0.9880599975585938|  0:00:13s\n",
      "epoch 21 | loss: 0.09238 | val_0_mse: 1.01596999168396|  0:00:13s\n",
      "epoch 22 | loss: 0.09708 | val_0_mse: 0.9950299859046936|  0:00:14s\n",
      "epoch 23 | loss: 0.10165 | val_0_mse: 0.5231800079345703|  0:00:15s\n",
      "epoch 24 | loss: 0.11204 | val_0_mse: 0.6743299961090088|  0:00:15s\n",
      "epoch 25 | loss: 0.1171  | val_0_mse: 0.7981500029563904|  0:00:16s\n",
      "epoch 26 | loss: 0.22428 | val_0_mse: 0.7183600068092346|  0:00:16s\n",
      "epoch 27 | loss: 0.07205 | val_0_mse: 0.5548200011253357|  0:00:17s\n",
      "epoch 28 | loss: 0.09213 | val_0_mse: 0.6716099977493286|  0:00:18s\n",
      "epoch 29 | loss: 0.07898 | val_0_mse: 0.5454400181770325|  0:00:18s\n",
      "epoch 30 | loss: 0.11606 | val_0_mse: 0.9801999926567078|  0:00:19s\n",
      "epoch 31 | loss: 0.07881 | val_0_mse: 0.807669997215271|  0:00:19s\n",
      "epoch 32 | loss: 0.08478 | val_0_mse: 0.9078199863433838|  0:00:20s\n",
      "epoch 33 | loss: 0.09703 | val_0_mse: 0.8151699900627136|  0:00:21s\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_mse = 0.5231800079345703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# Define the model\n",
    "model = TabNetRegressor(\n",
    "    n_d=8,\n",
    "    n_a=8,\n",
    "    n_steps=3,\n",
    "    gamma=1.3,\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    epsilon=1e-15,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train=X_train_tensor.numpy(),\n",
    "    y_train=y_train_tensor.numpy(),\n",
    "    eval_set=[(X_test_tensor.numpy(), y_test_tensor.numpy())],\n",
    "    batch_size=64,\n",
    "    virtual_batch_size=16,\n",
    "    num_workers=0,\n",
    "    max_epochs=100,\n",
    "    patience=10,  # Early stopping\n",
    "    drop_last=False  # Drop last incomplete batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.05162693187594414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_test_tensor.numpy())\n",
    "    \n",
    "    # Convert tensors to numpy arrays for evaluation\n",
    "    y_true = y_test_tensor.numpy()\n",
    "    \n",
    "    # Compute RMSE between log-transformed predictions and targets\n",
    "    epsilon = 1e-8\n",
    "    log_predictions = np.log(y_pred + epsilon)  # Apply log transformation\n",
    "    log_targets = np.log(y_true + epsilon)\n",
    "    mse = np.mean((log_predictions - log_targets) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Test RMSE: {rmse}')\n",
    "\n",
    "evaluate_model(model, X_test_tensor, y_test_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
